{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/anshupandey/Deploy_Machine_Learning_Projects/blob/master/Deploy_ML_Models_as_Service_with_FastAPI_%26_ColabCode_from_Colab.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nfkUNsC_T2qy"
      },
      "source": [
        "!pip install colabcode\n",
        "!pip install fastapi"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CjR3OXrZUWbN"
      },
      "source": [
        "from colabcode import ColabCode\n",
        "from fastapi import FastAPI"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "00z03aC9YVGk"
      },
      "source": [
        "cc = ColabCode(port=12000, code=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YFXBGHaNUk3h"
      },
      "source": [
        "app = FastAPI()\n",
        "\n",
        "@app.get(\"/\")\n",
        "async def read_root():\n",
        "  return {\"message\": \"Subscribe to @1littlecoder\"}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xzWXXJs-UmFO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fb5458fd-8a47-4630-94ac-a024f54bb6dc"
      },
      "source": [
        "cc.run_app(app=app)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Public URL: NgrokTunnel: \"http://ebd7bd6cabe1.ngrok.io\" -> \"http://localhost:12000\"\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:     Started server process [63]\n",
            "INFO:     Waiting for application startup.\n",
            "INFO:     Application startup complete.\n",
            "INFO:     Uvicorn running on http://127.0.0.1:12000 (Press CTRL+C to quit)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:     103.25.46.30:0 - \"GET / HTTP/1.1\" 200 OK\n",
            "INFO:     103.25.46.30:0 - \"GET /favicon.ico HTTP/1.1\" 404 Not Found\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:     Shutting down\n",
            "INFO:     Waiting for application shutdown.\n",
            "INFO:     Application shutdown complete.\n",
            "INFO:     Finished server process [63]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TE3gaKk7UpEV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9f1194e2-d5ca-416e-b7f5-ea0755c4e066"
      },
      "source": [
        "import numpy as np\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "import pickle\n",
        "\n",
        "iris = load_iris()\n",
        "model = GaussianNB()\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(iris.data, iris.target, test_size=0.1)\n",
        "model_f = model.fit(X_train, y_train)\n",
        "\n",
        "print(\"Model score: \", model.score(X_train, y_train))\n",
        "print(\"Test Accuracy: \", model.score(X_test, y_test))\n",
        "\n",
        "pickle.dump(model_f, open(\"model_gb.pkl\", \"wb\"))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model score:  0.9629629629629629\n",
            "Test Accuracy:  0.9333333333333333\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9sWUbNKGW9ZX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bd6a2a20-d262-453d-a56d-72dfed292608"
      },
      "source": [
        "%%writefile models.py\n",
        "from pydantic import BaseModel, conlist\n",
        "from typing import List\n",
        "\n",
        "\n",
        "class Iris(BaseModel):\n",
        "    data: List[conlist(float, min_items=4, max_items=4)]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Writing models.py\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RMpFW8OAV2r3"
      },
      "source": [
        "import pickle\n",
        "import logging\n",
        "from fastapi import FastAPI\n",
        "from models import Iris\n",
        "\n",
        "app = FastAPI(title=\"ML Models as API on Google Colab\", description=\"with FastAPI and ColabCode\", version=\"1.0\")\n",
        "\n",
        "# # Initialize logging\n",
        "# my_logger = logging.getLogger()\n",
        "# my_logger.setLevel(logging.DEBUG)\n",
        "# logging.basicConfig(level=logging.DEBUG, filename='logs.log')\n",
        "\n",
        "model = None\n",
        "\n",
        "@app.on_event(\"startup\")\n",
        "def load_model():\n",
        "    global model\n",
        "    model = pickle.load(open(\"model_gb.pkl\", \"rb\"))\n",
        "\n",
        "@app.post(\"/api\", tags=[\"prediction\"])\n",
        "async def get_predictions(iris: Iris):\n",
        "    try:\n",
        "        data = dict(iris)['data']\n",
        "        print(data)\n",
        "        iris_types = {\n",
        "            0: 'setosa',\n",
        "            1: 'versicolor',\n",
        "            2: 'virginica'\n",
        "        }\n",
        "        prediction = list(map(lambda x: iris_types[x], model.predict(data).tolist()))\n",
        "        log_proba = model.predict_log_proba(data).tolist()\n",
        "        return {\"prediction\": prediction, \"log_proba\": log_proba}\n",
        "    except:\n",
        "        my_logger.error(\"Something went wrong!\")\n",
        "        return {\"prediction\": \"error\"}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GFl9Ga5uW5ED",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1323e1aa-aa10-4341-8ce5-6948b98f0a13"
      },
      "source": [
        "cc.run_app(app=app)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Public URL: NgrokTunnel: \"http://a7ade6155418.ngrok.io\" -> \"http://localhost:12000\"\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:     Started server process [63]\n",
            "INFO:     Waiting for application startup.\n",
            "INFO:     Application startup complete.\n",
            "INFO:     Uvicorn running on http://127.0.0.1:12000 (Press CTRL+C to quit)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:     103.25.46.30:0 - \"GET / HTTP/1.1\" 404 Not Found\n",
            "INFO:     103.25.46.30:0 - \"GET /favicon.ico HTTP/1.1\" 404 Not Found\n",
            "INFO:     103.25.46.30:0 - \"GET /docs HTTP/1.1\" 200 OK\n",
            "INFO:     103.25.46.30:0 - \"GET /openapi.json HTTP/1.1\" 200 OK\n",
            "[[4.5, 1.5, 3.5, 2.5]]\n",
            "INFO:     103.25.46.30:0 - \"POST /api HTTP/1.1\" 200 OK\n",
            "[[1.5, 3.5, 3.5, 2.5]]\n",
            "INFO:     103.25.46.30:0 - \"POST /api HTTP/1.1\" 200 OK\n",
            "[[1.5, 5.5, 9.5, 2.5]]\n",
            "INFO:     103.25.46.30:0 - \"POST /api HTTP/1.1\" 200 OK\n",
            "[[1.5, 5.5, 9.5, 2.5]]\n",
            "INFO:     103.25.46.30:0 - \"POST /api HTTP/1.1\" 200 OK\n",
            "INFO:     103.25.46.30:0 - \"POST /api HTTP/1.1\" 422 Unprocessable Entity\n",
            "[[1.5, 5.5, 9.5, 2.5]]\n",
            "INFO:     103.25.46.30:0 - \"POST /api HTTP/1.1\" 200 OK\n",
            "[[1.5, 5.5, 9.5, 2.5]]\n",
            "INFO:     103.25.46.30:0 - \"POST /api HTTP/1.1\" 200 OK\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:     Shutting down\n",
            "INFO:     Waiting for application shutdown.\n",
            "INFO:     Application shutdown complete.\n",
            "INFO:     Finished server process [63]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9vnVzMeKXe5U"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    }
  ]
}